package datacoord

import (
	"context"

	"github.com/samber/lo"
	"go.uber.org/zap"

	"github.com/milvus-io/milvus/internal/datacoord/allocator"
	"github.com/milvus-io/milvus/internal/proto/datapb"
	"github.com/milvus-io/milvus/pkg/log"
)

// firstVshardCompactionPolicy split segments without vshardInfo into vshards according to vshardInfos generated by vshardManager
type firstVshardCompactionPolicy struct {
	meta          *meta
	allocator     allocator.Allocator
	handler       Handler
	vshardManager VshardManager
}

func newVshardSplitCompactionPolicy(
	meta *meta,
	allocator allocator.Allocator,
	handler Handler,
	vshardManager VshardManager) *firstVshardCompactionPolicy {
	return &firstVshardCompactionPolicy{
		meta:          meta,
		allocator:     allocator,
		handler:       handler,
		vshardManager: vshardManager,
	}
}

func (policy *firstVshardCompactionPolicy) Enable() bool {
	// todo wayblink add config
	return Params.DataCoordCfg.EnableAutoCompaction.GetAsBool()
}

func (policy *firstVshardCompactionPolicy) Trigger() (map[CompactionTriggerType][]CompactionView, error) {
	log.Info("start trigger first Vshard Split Compaction...")
	ctx := context.Background()
	collections := policy.meta.GetCollections()

	events := make(map[CompactionTriggerType][]CompactionView, 0)
	views := make([]CompactionView, 0)
	for _, collection := range collections {
		collectionInfo, err := policy.handler.GetCollection(ctx, collection.ID)
		if err != nil {
			log.Warn("fail to get collection from handler")
			return nil, err
		}
		if collectionInfo == nil {
			log.Warn("collection not exist")
			return nil, nil
		}
		if !isCollectionAutoCompactionEnabled(collectionInfo) {
			log.RatedInfo(20, "collection auto compaction disabled")
			return nil, nil
		}
		expectedSize := getExpectedSegmentSize(policy.meta, collectionInfo)

		for _, partitionID := range collectionInfo.Partitions {
			collectionViews, err := policy.triggerOnePartition(collectionInfo, partitionID, expectedSize)
			if err != nil {
				// not throw this error because no need to fail because of one collection
				log.Warn("fail to trigger single compaction", zap.Int64("collectionID", collection.ID), zap.Error(err))
			}
			views = append(views, collectionViews...)
		}
	}
	events[TriggerTypeFirstVshard] = views
	return events, nil
}

func (policy *firstVshardCompactionPolicy) triggerOnePartition(collectionInfo *collectionInfo, partitionID int64, expectedSize int64) ([]CompactionView, error) {
	log := log.With(zap.Int64("collectionID", collectionInfo.ID), zap.Int64("partitionID", partitionID))
	log.Info("start trigger vshard first split compaction")

	noVshardSegments := policy.meta.GetSegmentsChanPart(func(segment *SegmentInfo) bool {
		return segment.CollectionID == collectionInfo.ID &&
			segment.PartitionID == partitionID &&
			isSegmentHealthy(segment) &&
			isFlush(segment) &&
			!segment.isCompacting && // not compacting now
			!segment.GetIsImporting() && // not importing now
			segment.GetLevel() == datapb.SegmentLevel_L1 &&
			segment.GetVshardDesc() == nil
	})
	log.Info("Get segments without vshard", zap.Int("len", len(noVshardSegments)))

	// filter out no vshardInfo partition-channels to avoid useless AllocID
	noVshardSegments = lo.Filter(noVshardSegments, func(chanPartSegments *chanPartSegments, _ int) bool {
		partitionChVshardInfos := policy.vshardManager.GetActiveVShardInfos(partitionID, chanPartSegments.channelName)
		if len(partitionChVshardInfos) == 0 {
			log.Info("No vshardinfo in the partition-channel, skip trigger channelGroup split compaction")
			return false
		}
		return true
	})

	newTriggerID, _, err := policy.allocator.AllocN(int64(len(noVshardSegments)))
	if err != nil {
		log.Warn("fail to allocate triggerID", zap.Error(err))
		return nil, err
	}

	views := make([]CompactionView, 0)
	for _, channelGroup := range noVshardSegments {
		partitionChVshardInfos := policy.vshardManager.GetActiveVShardInfos(partitionID, channelGroup.channelName)
		if len(partitionChVshardInfos) == 0 {
			log.Info("No vshardinfo in the partition-channel, skip trigger channelGroup split compaction")
			continue
		}
		vshardDescs := lo.Map(partitionChVshardInfos, func(vshard *datapb.VShardInfo, _ int) *datapb.VShardDesc {
			return vshard.GetVshardDesc()
		})

		if Params.DataCoordCfg.IndexBasedCompaction.GetAsBool() {
			channelGroup.segments = FilterInIndexedSegments(policy.handler, policy.meta, channelGroup.segments...)
		}

		collectionTTL, err := getCollectionTTL(collectionInfo.Properties)
		if err != nil {
			log.Warn("get collection ttl failed, skip to handle compaction")
			return make([]CompactionView, 0), err
		}

		// group segments by size
		groups := make([][]*SegmentInfo, 0)
		var currentGroupSize int64 = 0
		currentGroup := make([]*SegmentInfo, 0)
		for _, segment := range channelGroup.segments {
			if currentGroupSize+segment.getSegmentSize() > expectedSize {
				groups = append(groups, currentGroup)
				currentGroup = make([]*SegmentInfo, 0)
				currentGroupSize = 0
			}
			currentGroupSize += segment.getSegmentSize()
			currentGroup = append(currentGroup, segment)
		}

		for _, segments := range groups {
			view := &VshardSegmentView{
				label: &CompactionGroupLabel{
					CollectionID: collectionInfo.ID,
					PartitionID:  partitionID,
					Channel:      channelGroup.channelName,
				},
				segments:      GetViewsByInfo(segments...),
				collectionTTL: collectionTTL,
				triggerID:     newTriggerID,
				fromVshards:   nil,
				toVshards:     vshardDescs,
			}
			views = append(views, view)
			segmentIDs := lo.Map(segments, func(seg *SegmentInfo, _ int) int64 { return seg.ID })
			log.Info("generate compaction view", zap.Int64s("segmentIDs", segmentIDs))
		}
		newTriggerID++
	}

	log.Info("finish trigger vshard first split compaction", zap.Int("viewNum", len(views)))
	return views, nil
}
